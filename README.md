# Autonomous Vehicles â€” Occupancy Grid Mapper

> A compact 2â€‘D occupancyâ€‘grid mapper for a teaching autonomous vehicle (MacAEV). LiDAR scans are fused with wheel/IMU odometry to estimate obstacle occupancy in real time and publish a `nav_msgs/OccupancyGrid` map.

---

## ğŸš— Overview

This project implements the classical **occupancyâ€‘grid mapping** pipeline on a small autonomous platform. The mapper consumes:

* **Localization:** wheel odometry for $x,y$ translation and **IMU yaw** for heading ($\theta$)
* **Perception:** 2â€‘D **LiDAR** scan returns (ranges and angles)

and produces a planar grid in the fixed **odometry** frame (`odom`) whose cells encode the probability of being **occupied**, **free**, or **unknown**.

**Big picture.** The vehicle drives, the LiDAR sends out many straightâ€‘line beams each cycle, and the mapper converts what those beams â€œsawâ€ into colored squares on a grid (free along the beam, occupied where it hits). Accumulating evidence over time sharpens the map.

Repository layout:

* `OccupancyGridMapping_.py` â€” ROS node (Python) that subscribes to Odometry and LaserScan, updates logâ€‘odds per cell, and publishes a `nav_msgs/OccupancyGrid`.
* `expermient_cpp_.txt` â€” wheelâ€‘odometry (C++) logic used in a prior lab; included for context.
* `Activity4_.bag` â€” example rosbag recorded during mapping (for playback/inspection).

---

## ğŸ”§ Platform & Frames (ROS tf)

**Quick ROS primer (context).**

* **ROS** provides a pub/sub system for robot data. Programs are **nodes**; they exchange typed **messages** on named **topics** (e.g., `/scan` for LiDAR, `/odom` for wheel+IMU pose).
* A **TF tree** tracks coordinate frames (e.g., `odom â†’ base_link â†’ laser`). TF lets any node transform points between frames with correct timing.
* A **map** in this project is a `nav_msgs/OccupancyGrid` (a 2â€‘D array with metadata).

**Frames used here.**

* **`odom`** â€” fixed world frame for the session (the grid is published here).
* **`base_link`** â€” body frame attached to the vehicleâ€™s chassis.
* **`laser`** â€” LiDAR frame (static transform from `base_link`).

**Pose state.** The vehicle pose in `odom` is $X=[x\ y\ \theta]^T$. Wheel odometry supplies $x,y$; IMU provides $\theta$.

![Fig 1 â€” Vehicle pose & frames (odom/base\_link/laser)](https://github.com/user-attachments/assets/557e761b-5474-4ca3-ae2d-1319fa63c426) <sub><b>Fig 1.</b> Coordinate frames and pose definition. The map is attached to <code>odom</code>; LiDAR originates in <code>laser</code>.</sub>

---

## ğŸ§­ Localization (Wheel + IMU yaw)

**What â€œodometryâ€ means here.** Odometry estimates pose by integrating measured motion: wheel travel gives a forward speed; steering sets the turn rate; IMU yaw provides absolute heading. Heading from the IMU avoids accumulating gyro drift and stabilizes geometry for mapping.

**Kinematic bicycle model (continuous time):**

$$
\dot x = v_s\cos\theta,\qquad
\dot y = v_s\sin\theta,\qquad
\dot\theta = \frac{v_s}{\ell}\tan\delta,
$$

with speed $v_s$, steering angle $\delta$, and wheelbase $\ell$.

**Discrete updates (Euler) with IMU yaw for heading:**

$$
\begin{aligned}
 x_{k+1}&=x_k + \Delta t_k\,v_{s,k}\cos\theta_k,\\
 y_{k+1}&=y_k + \Delta t_k\,v_{s,k}\sin\theta_k,\\
 \theta_k&=\text{yaw}^{\mathrm{IMU}}_{k}-\theta_0\, .
\end{aligned}
$$

The C++ excerpt in `expermient_cpp_.txt` publishes both a TF ($\text{odom}\to\text{base_link}$) and `nav_msgs/Odometry` with this fusion.

---

## ğŸ—ºï¸ Occupancy Grid Mapping â€” Intuition first

An **occupancy grid** divides the plane into small cells (meters per cell). Each cell stores a belief $p(m_{ij})$ that something solid occupies that square.

**How LiDAR evidence maps to cells.** For any cell center, the algorithm picks the **closest LiDAR beam** to that direction and reasons:

* along the beam **before** the hit â†’ **free**;
* the **cell containing the hit** â†’ **occupied**;
* just beyond the hit â†’ **unknown**.

![Fig 2 â€” Grid in odom, LiDAR rays in laser, and projection to cells](https://github.com/user-attachments/assets/fb3e2a15-66c8-4108-8ccc-a82362c7f249) <sub><b>Fig 2.</b> Cells are updated by projecting LiDAR rays from <code>laser</code> into the grid in <code>odom</code>.</sub>

![Fig 3 â€” Inverse sensor model: alongâ€‘ray cells free, hit cell occupied](https://github.com/user-attachments/assets/616cf8ff-7911-4044-98bc-3daf7c56bf57) <sub><b>Fig 3.</b> Inverse sensor model: free along the ray (before range), occupied at the terminal cell, unknown past the hit.</sub>

---

## ğŸ§® From probabilities to logâ€‘odds (and back)

Directly adding probabilities is awkward. Each cell instead holds **logâ€‘odds**

$$
\ell=\log\frac{p}{1-p}\, .
$$

The recursive update per cell is

$$
\ell_t\big(m_{ij}\big) = \ell_{t-1}\big(m_{ij}\big) + \underbrace{\ell\big(m_{ij}\mid z_t,x_t\big)}_{\text{inverse sensor contribution}} - \ell_0\big(m_{ij}\big)\, .
$$

With an uninformative prior ($p_0 = 0.5 \Rightarrow \ell_0 = 0$), this becomes adding a negative constant for **free** cells and a positive constant for **occupied** cells, with clamping to $[\ell_{\min},\ell_{\max}]$. Probability recovery uses

$$
 p = \frac{1}{1+e^{-\ell}}\, .
$$

---

## ğŸ§© Mapper pipeline (what the node actually does)

1. **Subscribe** to `nav_msgs/Odometry` (pose in `odom`) and `sensor_msgs/LaserScan` (ranges $r_n$, angles $\alpha_n$).
2. **Cache pose** $x_k$ on odom callback; **process scan** on laser callback.
3. For each **cell center** $c_{ij}$: transform to the LiDAR frame, select nearest beam, compare distance $d=\lVert c_{\text{laser}}\rVert$ to measured range $r$, and add the appropriate logâ€‘odds increment (free / occupied / unknown). Clamp values.
4. **Publish** `nav_msgs/OccupancyGrid`: set `info.resolution`, `info.width/height`, `info.origin` (pose of cell (0,0) in `odom`), and rowâ€‘major `data` where **100** = occupied, **0** = free, **âˆ’1** = unknown.

### Minimal pseudocode (annotated)

```python
# OccupancyGridMapping_.py (sketch)
def laser_cb(scan):
    pose = latest_odom_pose  # (x, y, theta) in 'odom'
    T_odom_laser = tf_odom_to_laser(pose)
    for (i, j) in all_grid_cells:
        c_odom  = origin_xy + np.array([i, j]) * res
        c_laser = inv(T_odom_laser) @ to_homog(c_odom)
        phi     = atan2(c_laser.y, c_laser.x)
        n       = nearest_index(phi, scan.angle_min, scan.angle_increment)
        r       = scan.ranges[n]
        d       = hypot(c_laser.x, c_laser.y)
        if hit_inside_cell(r, d, res):
            L[i, j] = clamp(L[i, j] + logit(p_occ))
        elif r > d + margin:
            L[i, j] = clamp(L[i, j] + logit(p_free))
        # else unknown
    publish_grid(L)
```

---

## ğŸ§ª Expected behavior (qualitative)

Observed during lab playback and consistent with the documents:

* **Freeâ€‘space fans** accumulate as the platform moves, carving corridors of free cells.
* **Obstacle rims** appear at beam terminations (walls, carts, cardboard).
* **Unknown regions** persist where no beam ever passed (behind obstacles, unobserved corners).
* **Heading stabilization** from IMU yaw reduces rotational drift; small translation drift may misalign loop closures (expected without scanâ€‘matching).
* **Resolution tradeâ€‘off:** $0.05\!\text{ m}$â€“$0.10\!\text{ m}$ per cell balances crispness vs. compute.

Suggested visuals to add later (replace with real uploads):

```markdown
![RViz â€” live mapping](RVIZ_SCREENSHOT_URL)
![Final occupancy grid](FINAL_MAP_URL)
```

---

## ğŸ§° Practical parameters (typical)

* `resolution`: **0.05â€“0.10 m/cell**
* `p_occ`: **0.65â€“0.75**  â†’ $\ell_{\text{occ}} = \log\!\frac{p_{\text{occ}}}{1-p_{\text{occ}}}$
* `p_free`: **0.30â€“0.45** â†’ $\ell_{\text{free}} = \log\!\frac{p_{\text{free}}}{1-p_{\text{free}}}$ (negative)
* `l_min`, `l_max`: **\[âˆ’4.0, +4.0]**
* `margin`: \~ **0.5 Ã— resolution**
* `frame_id`: `'odom'` for the published grid; ensure TF tree is consistent

---

## ğŸ§¾ Message & data conventions

* Grid indexing is **rowâ€‘major**: `data[row*width + col]`.
* **Origin** (`info.origin`) stores the pose of cell (0,0) in `odom`. Sliding windows require updating this origin.
* **Unknown** cells (âˆ’1) remain until evidence arrives; no prior is injected.

---

## ğŸ—£ï¸ Glossary (quick context)

* **LiDAR beam** â€” a ray with known angle and measured range.
* **Inverse sensor model** â€” rule mapping a beamâ€™s geometry to cell beliefs.
* **Logâ€‘odds** â€” $\ell=\log\tfrac{p}{1-p}$; turns probabilistic fusion into addition.
* **Bresenham / rayâ€‘tracing** â€” integer grid traversal visiting all crossed cells.
* **TF** â€” ROS transform system between frames (e.g., `odomâ†’base_linkâ†’laser`).

---

## ğŸ“š How this ties together

1. **Pose** from wheel+IMU determines the LiDAR origin in `odom`.
2. **Each scan** is a bundle of rays; intersecting them with the grid yields free and occupied evidence.
3. **Logâ€‘odds** accumulate evidence over time, resisting noise and enabling correction.
4. The published **OccupancyGrid** feeds planners, localizers, and RViz visualization.

---

## License

Released under the **MIT License** â€” see [LICENSE](LICENSE).
